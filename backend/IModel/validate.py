from ultralytics import YOLO
from stream_to_logger import StreamToLogger
import logging
import sys
import os
import yaml
import time
import json
from collections.abc import Mapping, Sequence


def _to_plain_python(obj):
    """
    å°†å¯èƒ½åŒ…å« numpy / torch ç­‰å¯¹è±¡çš„ç»“æ„è½¬æ¢ä¸ºçº¯ Python åŸºæœ¬ç±»å‹ï¼Œä¾¿äº yaml.safe_dump / json åºåˆ—åŒ–ã€‚
    """
    # Lazy imports to avoid hard deps
    np = None
    torch = None
    try:
        import numpy as _np  # type: ignore
        np = _np
    except Exception:
        pass
    try:
        import torch as _torch  # type: ignore
        torch = _torch
    except Exception:
        pass

    # 1) åŸºç¡€æ ‡é‡
    if isinstance(obj, (str, int, float, bool)) or obj is None:
        return obj

    # 2) numpy æ ‡é‡
    if np is not None and isinstance(obj, np.generic):
        try:
            return obj.item()
        except Exception:
            # å…œåº•è½¬ä¸º float æˆ–å­—ç¬¦ä¸²
            try:
                return float(obj)
            except Exception:
                return str(obj)

    # 3) torch å¼ é‡
    if torch is not None:
        # 0-dim tensor -> æ ‡é‡
        if isinstance(obj, torch.Tensor):
            try:
                if obj.ndim == 0:
                    return _to_plain_python(obj.item())
                else:
                    return _to_plain_python(obj.detach().cpu().tolist())
            except Exception:
                try:
                    return _to_plain_python(obj.tolist())
                except Exception:
                    return str(obj)

    # 4) numpy æ•°ç»„ï¼ˆæ”¾åœ¨åºåˆ—åˆ¤æ–­å‰ï¼Œé¿å…èµ°åˆ°é€šç”¨åºåˆ—åˆ†æ”¯ï¼‰
    if np is not None:
        try:
            if isinstance(obj, np.ndarray):
                return _to_plain_python(obj.tolist())
        except Exception:
            pass

    # 5) æ˜ å°„ç±»å‹
    if isinstance(obj, Mapping):
        try:
            return { str(k): _to_plain_python(v) for k, v in obj.items() }
        except Exception:
            # å°½é‡é€é¡¹è½¬æ¢ï¼Œå¤±è´¥åˆ™å­—ç¬¦ä¸²åŒ–
            out = {}
            for k, v in list(getattr(obj, 'items', lambda: [])()):
                try:
                    out[str(k)] = _to_plain_python(v)
                except Exception:
                    out[str(k)] = str(v)
            return out

    # 6) åºåˆ—ï¼ˆä½†æ’é™¤å­—èŠ‚/å­—ç¬¦ä¸²ï¼‰
    if isinstance(obj, Sequence) and not isinstance(obj, (str, bytes, bytearray)):
        try:
            return [_to_plain_python(x) for x in obj]
        except Exception:
            # é€ä¸ªå…ƒç´ å…œåº•
            out_list = []
            for x in obj:
                try:
                    out_list.append(_to_plain_python(x))
                except Exception:
                    out_list.append(str(x))
            return out_list

    # 7) å…¶ä»–å¯è½¬æ¢ä¸º float çš„å¯¹è±¡
    try:
        return float(obj)
    except Exception:
        # æœ€åå…œåº•è½¬å­—ç¬¦ä¸²
        return str(obj)


def validate_model(model_path, dataset_yaml_path, output_dir, result_file_path, logger=None, task_id=None):
    if logger is None:
        logger = logging.getLogger("default")
        logger.setLevel(logging.INFO)
        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter("[%(asctime)s] %(message)s"))
        logger.addHandler(handler)

    if task_id is None:
        logger.info("æœªè·å–åˆ°Task ID")
        sys.exit(1)

    if result_file_path is None:
        logger.info("æœªè·å–åˆ°Validation Result File Path")
        sys.exit(1)

    log_cache = []

    stdout_backup = sys.stdout
    stderr_backup = sys.stderr
    sys.stdout = StreamToLogger(logger, logging.INFO, log_cache=log_cache)
    sys.stderr = StreamToLogger(logger, logging.ERROR, log_cache=log_cache)

    logger.info("[INFO] å¯åŠ¨éªŒè¯ä»»åŠ¡...")

    try:
        model = YOLO(model_path)
        # è¦†ç›–æ¨¡å‹ç±»åï¼Œç¡®ä¿ç»˜å›¾ï¼ˆåŒ…æ‹¬æ··æ·†çŸ©é˜µï¼‰ä½¿ç”¨æ•°æ®é›†ç±»åˆ«å
        save_json_flag = True  # é»˜è®¤å¯¼å‡º JSON
        plots_flag = True      # é»˜è®¤ç”Ÿæˆå›¾è¡¨
        class_mismatch = False
        ds_names_list = None
        try:
            if isinstance(dataset_yaml_path, str) and os.path.exists(dataset_yaml_path):
                with open(dataset_yaml_path, 'r', encoding='utf-8') as f_yaml_for_names:
                    ds_yaml_for_names = yaml.safe_load(f_yaml_for_names) or {}
                ds_names = ds_yaml_for_names.get('names', None)
                names_dict = None
                if isinstance(ds_names, list):
                    ds_names_list = list(ds_names)
                    names_dict = {i: n for i, n in enumerate(ds_names)}
                elif isinstance(ds_names, dict):
                    names_dict = ds_names
                    # å°è¯•è½¬ä¸ºæœ‰åºåˆ—è¡¨
                    try:
                        ds_names_list = [names_dict[i] for i in range(len(names_dict))]
                    except Exception:
                        ds_names_list = list(names_dict.values())
                if names_dict:
                    try:
                        model.names = names_dict  # ä¼˜å…ˆè®¾ç½®é«˜çº§å°è£…å±æ€§
                    except Exception:
                        pass
                    try:
                        # åŒæ—¶å°è¯•è®¾ç½®åº•å±‚æ¨¡å‹å±æ€§ï¼Œå¢å¼ºå…¼å®¹æ€§
                        if hasattr(model, 'model') and hasattr(model.model, 'names'):
                            model.model.names = names_dict
                    except Exception:
                        pass
                # å¦‚æœæ•°æ®é›†ç±»åˆ«æ•°ä¸æ¨¡å‹ç±»åˆ«æ•°ä¸ä¸€è‡´ï¼Œç¦ç”¨ save_json ä»¥é¿å…å†…éƒ¨ class_map è¶Šç•Œ
                try:
                    model_nc = None
                    if hasattr(model, 'model') and hasattr(model.model, 'nc'):
                        model_nc = int(model.model.nc)
                    elif hasattr(model, 'names') and isinstance(model.names, (dict, list)):
                        model_nc = len(model.names)
                    ds_nc = len(ds_names_list) if isinstance(ds_names_list, list) else None
                    if isinstance(model_nc, int) and isinstance(ds_nc, int) and model_nc != ds_nc:
                        class_mismatch = True
                        save_json_flag = False
                        plots_flag = False  # é¿å… ConfusionMatrix åœ¨å†…éƒ¨ä»¥æ•°æ®é›†ç±»åˆ«æ•°æ„å»ºçŸ©é˜µæ—¶è¶Šç•Œ
                        logger.warning(f"[WARN] æ¨¡å‹ç±»åˆ«æ•°({model_nc})ä¸æ•°æ®é›†ç±»åˆ«æ•°({ds_nc})ä¸ä¸€è‡´ï¼Œå·²ç¦ç”¨ save_json ä¸å›¾è¡¨ç»˜åˆ¶ä»¥é¿å…å´©æºƒã€‚")
                except Exception:
                    pass
        except Exception:
            # å¦‚æœæ— æ³•è¦†ç›–ï¼Œä¸å½±å“éªŒè¯æµç¨‹
            pass

        # Run validation
        metrics = model.val(
            data=dataset_yaml_path,
            project=output_dir,
            name="result",
            save_json=save_json_flag,
            plots=plots_flag,
        )
    finally:
        sys.stdout = stdout_backup
        sys.stderr = stderr_backup

    logger.info("[INFO] ğŸ‰ éªŒè¯å®Œæˆ")

    completed_time = int(time.time())

    # ä»…ä¿ç•™æ—¥å¿—åŸæ–‡ä»¥ä¾¿å›æ”¾ï¼Œä¸å†ä»æ—¥å¿—è§£æ per-class è¡¨æ ¼

    result_info = {
        "completedAt": completed_time,
        "log": log_cache
    }

    # è‹¥ç±»åˆ«æ•°ä¸ä¸€è‡´ï¼Œè®°å…¥ç»“æœï¼Œä¾›å‰ç«¯å±•ç¤ºå‹å¥½æç¤º
    try:
        if class_mismatch:
            result_info['class_mismatch'] = True
            # å†™å…¥ç®€å•çš„æ•°é‡ä¿¡æ¯ï¼ˆè‹¥å¯ç”¨ï¼‰
            try:
                result_info['dataset_nc'] = len(ds_names_list) if isinstance(ds_names_list, list) else None
            except Exception:
                pass
            try:
                if hasattr(model, 'model') and hasattr(model.model, 'nc'):
                    result_info['model_nc'] = int(model.model.nc)
                elif hasattr(model, 'names') and isinstance(model.names, (dict, list)):
                    result_info['model_nc'] = len(model.names)
            except Exception:
                pass
    except Exception:
        pass

    # Try to append simple metrics and per-class rows from metrics (preferred over parsing logs)
    try:
        # Overall metrics summary (dict)
        raw_metrics = None
        if hasattr(metrics, 'results_dict'):
            raw_metrics = getattr(metrics, 'results_dict')
        else:
            # Fallback minimal overall
            overall = {}
            try:
                if hasattr(metrics, 'box'):
                    if hasattr(metrics.box, 'map50'):
                        overall['mAP50'] = float(metrics.box.map50)
                    if hasattr(metrics.box, 'map'):
                        overall['mAP50_95'] = float(metrics.box.map)
                    if hasattr(metrics.box, 'mp'):
                        overall['P'] = float(metrics.box.mp)
                    if hasattr(metrics.box, 'mr'):
                        overall['R'] = float(metrics.box.mr)
            except Exception:
                pass
            if overall:
                raw_metrics = overall
        if raw_metrics is not None:
            result_info["metrics"] = _to_plain_python(raw_metrics)

        # Per-class rows
        per_class_rows_by_metrics = []
        try:
            # 1) ä¼˜å…ˆä»æ•°æ®é›† YAML ä¸­è·å–æƒå¨ç±»åˆ«åé¡ºåº
            names_list = None
            # é¢„å…ˆå‡†å¤‡åŸºäºæ•°æ®é›†ç»Ÿè®¡åˆ°çš„æŒ‰ç±»å›¾ç‰‡/å®ä¾‹æ•°é‡
            images_per_class = None
            instances_per_class = None
            total_val_images = None
            try:
                if isinstance(dataset_yaml_path, str) and os.path.exists(dataset_yaml_path):
                    with open(dataset_yaml_path, 'r', encoding='utf-8') as f_yaml:
                        ds_yaml = yaml.safe_load(f_yaml) or {}
                    ds_names = ds_yaml.get('names', None)
                    # è§£æ val æ ‡ç­¾ç›®å½•ï¼Œç»Ÿè®¡æ¯ç±»å›¾ç‰‡æ•°ä¸å®ä¾‹æ•°
                    try:
                        # root é»˜è®¤ä¸º yaml æ‰€åœ¨ç›®å½•
                        root_dir = ds_yaml.get('path')
                        if not root_dir:
                            root_dir = os.path.dirname(os.path.abspath(dataset_yaml_path))
                        val_path = ds_yaml.get('val')
                        if isinstance(val_path, str):
                            # ä»…æ”¯æŒæœ¬åœ°è·¯å¾„ï¼ˆç›¸å¯¹æˆ–ç»å¯¹ï¼‰
                            if os.path.isabs(val_path):
                                images_dir = val_path
                            else:
                                images_dir = os.path.join(root_dir, val_path)
                            # æ¨å¯¼ labels ç›®å½•
                            labels_dir = images_dir.replace('/images', '/labels').replace('\\images', '\\labels')
                            if os.path.isdir(images_dir):
                                # ç»Ÿè®¡æ€»å›¾ç‰‡æ•°
                                exts = ('.jpg', '.jpeg', '.png', '.webp')
                                total_val_images = sum(1 for fn in os.listdir(images_dir) if fn.lower().endswith(exts))
                            # ç»Ÿè®¡æ¯ç±»å®ä¾‹ä¸åŒ…å«è¯¥ç±»çš„å›¾ç‰‡æ•°é‡
                            if ds_names is not None and os.path.isdir(labels_dir):
                                ncls_est = len(ds_names) if isinstance(ds_names, (list, dict)) else None
                                if isinstance(ncls_est, int) and ncls_est > 0:
                                    images_per_class = [0] * ncls_est
                                    instances_per_class = [0] * ncls_est
                                    # éå† labels ä¸‹æ‰€æœ‰ .txt
                                    for root_d, _, files in os.walk(labels_dir):
                                        for f in files:
                                            if not f.lower().endswith('.txt'):
                                                continue
                                            fpath = os.path.join(root_d, f)
                                            try:
                                                with open(fpath, 'r', encoding='utf-8') as lf:
                                                    lines = [ln.strip() for ln in lf.readlines() if ln.strip()]
                                            except Exception:
                                                lines = []
                                            # æŒ‰å›¾ç‰‡ç»Ÿè®¡æ˜¯å¦å‡ºç°è¿‡è¯¥ç±»
                                            has_class = set()
                                            for ln in lines:
                                                try:
                                                    cls_idx = int(ln.split()[0])
                                                except Exception:
                                                    continue
                                                if 0 <= cls_idx < ncls_est:
                                                    instances_per_class[cls_idx] += 1
                                                    has_class.add(cls_idx)
                                            for cls_idx in has_class:
                                                images_per_class[cls_idx] += 1
                    except Exception:
                        images_per_class = None
                        instances_per_class = None
                        total_val_images = None
                    # names å¯ä»¥æ˜¯ list æˆ– dictï¼ˆid->nameï¼‰
                    if isinstance(ds_names, dict):
                        try:
                            names_list = [ds_names[i] for i in range(len(ds_names))]
                        except Exception:
                            names_list = list(ds_names.values())
                    elif isinstance(ds_names, list):
                        names_list = ds_names
            except Exception:
                names_list = None

            # 2) è‹¥æ•°æ®é›†æœªæä¾›ï¼Œåˆ™å›é€€åˆ° metrics / model çš„ names
            if names_list is None:
                names = None
                if hasattr(metrics, 'names') and isinstance(getattr(metrics, 'names'), (list, dict)):
                    names = getattr(metrics, 'names')
                elif 'names' in raw_metrics if isinstance(raw_metrics, dict) else False:
                    names = raw_metrics['names']
                # Fallback to model names
                try:
                    if names is None and hasattr(model, 'names'):
                        names = model.names
                except Exception:
                    pass

                if isinstance(names, dict):
                    try:
                        names_list = [names[i] for i in range(len(names))]
                    except Exception:
                        names_list = list(names.values())
                elif isinstance(names, list):
                    names_list = names

            ncls = len(names_list) if names_list else None

            # 3) æ”¶é›†å„ç±»æŒ‡æ ‡æ•°ç»„ï¼ˆä¸åŒç‰ˆæœ¬å­—æ®µåä¸åŒï¼Œåšå…¼å®¹ï¼‰
            def _as_list(arr):
                try:
                    return list(arr) if arr is not None else None
                except Exception:
                    return None

            p_list = _as_list(getattr(metrics.box, 'p', None) if hasattr(metrics, 'box') else None)
            r_list = _as_list(getattr(metrics.box, 'r', None) if hasattr(metrics, 'box') else None)
            nt_list = _as_list(getattr(metrics.box, 'nt', None) if hasattr(metrics, 'box') else None)  # number of targets per class
            map50_list = None
            map5095_list = None

            for cand in ['maps', 'map50_per_class', 'ap50', 'ap50_per_class']:
                if hasattr(metrics, 'box') and hasattr(metrics.box, cand):
                    map50_list = _as_list(getattr(metrics.box, cand))
                    if map50_list:
                        break
            for cand in ['map_per_class', 'ap', 'ap_per_class']:
                if hasattr(metrics, 'box') and hasattr(metrics.box, cand):
                    map5095_list = _as_list(getattr(metrics.box, cand))
                    if map5095_list:
                        break

            # 4) å¯¹é½é•¿åº¦ï¼ˆä»¥ç±»åˆ«æ•°ä¸ºå‡†ï¼‰
            def _norm_len(lst, L):
                if lst is None or L is None:
                    return lst
                if len(lst) > L:
                    return lst[:L]
                if len(lst) < L:
                    return lst + [None] * (L - len(lst))
                return lst

            if ncls:
                p_list = _norm_len(p_list, ncls)
                r_list = _norm_len(r_list, ncls)
                map50_list = _norm_len(map50_list, ncls)
                map5095_list = _norm_len(map5095_list, ncls)
                nt_list = _norm_len(nt_list, ncls)

            # 5) ç”Ÿæˆè¡Œï¼Œä¸¥æ ¼æŒ‰ç…§ names_list é¡ºåºè¾“å‡ºï¼Œé¿å…ç±»åˆ«é”™ä½
            if names_list and (p_list or r_list or map50_list or map5095_list):
                for i, cname in enumerate(names_list):
                    row = {
                        'Class': cname,
                        'Images': None,
                        'Instances': None,
                    }
                    # ä¼˜å…ˆå¡«å……æ¥è‡ªæ•°æ®é›†çš„ç»Ÿè®¡
                    try:
                        if images_per_class is not None and i < len(images_per_class):
                            row['Images'] = int(images_per_class[i])
                    except Exception:
                        pass
                    try:
                        if instances_per_class is not None and i < len(instances_per_class):
                            row['Instances'] = int(instances_per_class[i])
                    except Exception:
                        pass
                    if p_list and i < len(p_list):
                        try: row['P'] = float(p_list[i])
                        except Exception: row['P'] = p_list[i]
                    if r_list and i < len(r_list):
                        try: row['R'] = float(r_list[i])
                        except Exception: row['R'] = r_list[i]
                    # Derive F1 when possible
                    try:
                        if isinstance(row.get('P'), (int, float)) and isinstance(row.get('R'), (int, float)):
                            p_v, r_v = float(row['P']), float(row['R'])
                            denom = (p_v + r_v)
                            if denom > 0:
                                row['F1'] = 2 * p_v * r_v / denom
                    except Exception:
                        pass
                    if nt_list and i < len(nt_list):
                        try: row['Instances'] = int(nt_list[i])
                        except Exception: row['Instances'] = nt_list[i]
                    if map50_list and i < len(map50_list):
                        try: row['mAP50'] = float(map50_list[i])
                        except Exception: row['mAP50'] = map50_list[i]
                    if map5095_list and i < len(map5095_list):
                        try: row['mAP50_95'] = float(map5095_list[i])
                        except Exception: row['mAP50_95'] = map5095_list[i]
                    per_class_rows_by_metrics.append(row)

            # Add overall row 'all'
            overall_row = {'Class': 'all', 'Images': None, 'Instances': None}
            # æ€»å›¾ç‰‡/å®ä¾‹æ•°ï¼ˆæ¥è‡ªæ•°æ®é›†ç»Ÿè®¡ï¼‰
            try:
                if total_val_images is not None:
                    overall_row['Images'] = int(total_val_images)
            except Exception:
                pass
            try:
                if instances_per_class is not None:
                    overall_row['Instances'] = int(sum(instances_per_class))
            except Exception:
                pass
            try:
                if hasattr(metrics, 'box'):
                    if hasattr(metrics.box, 'mp'):
                        overall_row['P'] = float(metrics.box.mp)
                    if hasattr(metrics.box, 'mr'):
                        overall_row['R'] = float(metrics.box.mr)
                    if hasattr(metrics.box, 'map50'):
                        overall_row['mAP50'] = float(metrics.box.map50)
                    if hasattr(metrics.box, 'map'):
                        overall_row['mAP50_95'] = float(metrics.box.map)
                    # Derive overall F1 when possible
                    try:
                        if isinstance(overall_row.get('P'), (int, float)) and isinstance(overall_row.get('R'), (int, float)):
                            p_v, r_v = float(overall_row['P']), float(overall_row['R'])
                            denom = (p_v + r_v)
                            if denom > 0:
                                overall_row['F1'] = 2 * p_v * r_v / denom
                    except Exception:
                        pass
            except Exception:
                pass
            # ä»…åœ¨è‡³å°‘æœ‰ä¸€ä¸ªå­—æ®µæ—¶åŠ å…¥
            if any(k in overall_row for k in ['P','R','mAP50','mAP50_95']):
                per_class_rows_by_metrics.insert(0, overall_row)

        except Exception:
            per_class_rows_by_metrics = []

        if per_class_rows_by_metrics:
            result_info['per_class_rows'] = _to_plain_python(per_class_rows_by_metrics)
            # åŒæ—¶æŒä¹…åŒ–ç±»åˆ«åï¼Œå‰ç«¯/åç«¯åç»­å¯ç›´æ¥ä½¿ç”¨
            try:
                result_info['names'] = _to_plain_python([r['Class'] for r in per_class_rows_by_metrics if 'Class' in r and r['Class'] != 'all'])
            except Exception:
                pass
    except Exception:
        pass

    if os.path.exists(result_file_path):
        with open(result_file_path, 'r', encoding='utf-8') as f:
            existing_data = yaml.safe_load(f) or {}
    else:
        existing_data = {}

    existing_data.update(result_info)

    # ä¸ä»æ—¥å¿—å†™å…¥ per_class_table / per_class_rowsï¼›per_class_rows ç”± metrics ç”Ÿæˆ

    # Ensure everything is JSON/YAML-safe
    safe_data = _to_plain_python(existing_data)

    # å¯é å†™å…¥ YAMLï¼šè‹¥ safe_dump ä»å¤±è´¥ï¼Œè¿›è¡Œä¸€æ¬¡æ›´ä¸¥æ ¼çš„æ·±åº¦æ¸…æ´—åé‡è¯•
    try:
        with open(result_file_path, 'w', encoding='utf-8') as f:
            yaml.safe_dump(safe_data, f, allow_unicode=True)
    except Exception:
        # æ·±åº¦æ¸…æ´—ï¼ˆå†æ¬¡è·‘ä¸€éï¼Œç¡®ä¿æç«¯å¯¹è±¡è¢«å¼ºåˆ¶è½¬ str/floatï¼‰
        def deep_sanitize(x):
            x = _to_plain_python(x)
            if isinstance(x, dict):
                return {str(k): deep_sanitize(v) for k, v in x.items()}
            if isinstance(x, list):
                return [deep_sanitize(v) for v in x]
            return x

        really_safe = deep_sanitize(safe_data)
        try:
            with open(result_file_path, 'w', encoding='utf-8') as f:
                yaml.safe_dump(really_safe, f, allow_unicode=True)
        except Exception:
            # æœ€ç»ˆå…œåº•ï¼šé€šè¿‡ JSON round-trip å¼ºåˆ¶è½¬ä¸ºåŸç”Ÿå¯åºåˆ—åŒ–ç±»å‹
            def _json_default(o):
                try:
                    # ä¼˜å…ˆå°è¯•è½¬æ¢ä¸ºåŸç”Ÿç±»å‹
                    return _to_plain_python(o)
                except Exception:
                    try:
                        return float(o)
                    except Exception:
                        return str(o)

            try:
                json_text = json.dumps(really_safe, ensure_ascii=False, default=_json_default)
                final_safe = json.loads(json_text)
            except Exception:
                # å¦‚æœä¾ç„¶å¤±è´¥ï¼Œé€€åŒ–ä¸ºå°†æ•´ä¸ªå¯¹è±¡è½¬å­—ç¬¦ä¸²
                final_safe = str(really_safe)

            with open(result_file_path, 'w', encoding='utf-8') as f:
                yaml.safe_dump(final_safe, f, allow_unicode=True)
